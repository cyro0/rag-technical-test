{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain dependencies\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader # Importing PDF loader from Langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Importing text splitter from Langchain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings # Importing Huggingface embeddings from Langchain\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint # Import Huggingface Chat models from Langchain \n",
    "from langchain.schema import Document # Importing Document schema from Langchain\n",
    "from langchain.vectorstores.chroma import Chroma # Import Chroma vector store from Langchain\n",
    "# from langchain_chroma import Chroma # Import Chroma vector store from Langchain\n",
    "from langchain_core.prompts import ChatPromptTemplate # Import Chat Prompt Template from Langchain\n",
    "from dotenv import load_dotenv # Importing dotenv to get API key from .env file\n",
    "import os # Importing os module for operating system functionalities\n",
    "import shutil # Importing shutil module for high-level file operations\n",
    "import getpass # Importing getpass module to transfer API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your API Key from HuggingFace\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Einsatz von kleinen Sprachmodellen zur\n",
      "Detektion von böswilligen Prompts\n",
      "Ciro Vincenzo Cascone\n",
      "Bachelor-Thesis\n",
      "zur Erlangung des akademischen Grades Bachelor of Science (B.Sc.)\n",
      "Studiengang Informatik\n",
      "Fakultät für Informatik\n",
      "Hochschule Mannheim\n",
      "14.10.2024\n",
      "Betreuer\n",
      "Prof. Dr. Jörn Fischer, Hochschule Mannheim\n",
      "Prof. Dr. rer. nat. Kai Eckert, Hochschule Mannheim\n",
      "' metadata={'source': 'data\\\\Bachelorarbeit_CiroCascone_2023392_IB8.pdf', 'page': 0, 'page_label': ''}\n"
     ]
    }
   ],
   "source": [
    "# Directory to pdf files:\n",
    "DATA_PATH = 'data'\n",
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load PDF Docs from specified directory in DATA_PATH\n",
    "    return:\n",
    "    Loaded PDF represented as Langchain Document objects\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize PDF loader with specified directory\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    # Load PDF Docs and return them as a list of Document objects\n",
    "    return document_loader.load()\n",
    "\n",
    "documents = load_documents()\n",
    "# Inspect contents of the first document as well as the metadata\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(documents: list[Document]):\n",
    "    \"\"\"\n",
    "    Split text content of given list of Documents into smaller chunks\n",
    "    args:\n",
    "    document (list[Document]): List of Document objects containing text content\n",
    "    return:\n",
    "    list[Document]: List of Document objects representing the split chunks\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize text splitter with following parameters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300, # Size of each chunk in characters\n",
    "        chunk_overlap=100, # Overlap between consecutive chunks\n",
    "        length_function=len, # Function to compute length of given text\n",
    "        add_start_index=True # Flag to add start index to each chunk\n",
    "    )\n",
    "\n",
    "    # Split documents into smaller chunks using text splitter\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "\n",
    "    # Print example of page content and metadata for a chunk\n",
    "    document = chunks[0]\n",
    "    print(f\"Example of chunk: \\n {document.page_content} \\n \\n {document.metadata}\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to directory to save Chroma database\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    \"\"\"\n",
    "    Save a given list of Document objects to the Chroma database.\n",
    "    args:\n",
    "    chunks (list[Document]): List of Document objects representing text chunks to save\n",
    "    returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Clear out database directory if it already exists\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create new Chroma database from the documents provided\n",
    "    db = Chroma.from_documents(\n",
    "        chunks,\n",
    "        HuggingFaceEmbeddings(),\n",
    "        persist_directory = CHROMA_PATH\n",
    "    )\n",
    "\n",
    "    # Persist the database to disk\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 72 documents into 698 chunks\n",
      "Example of chunk: \n",
      " Einsatz von kleinen Sprachmodellen zur\n",
      "Detektion von böswilligen Prompts\n",
      "Ciro Vincenzo Cascone\n",
      "Bachelor-Thesis\n",
      "zur Erlangung des akademischen Grades Bachelor of Science (B.Sc.)\n",
      "Studiengang Informatik\n",
      "Fakultät für Informatik\n",
      "Hochschule Mannheim\n",
      "14.10.2024\n",
      "Betreuer \n",
      " \n",
      " {'source': 'data\\\\Bachelorarbeit_CiroCascone_2023392_IB8.pdf', 'page': 0, 'page_label': '', 'start_index': 0}\n",
      "Saved 698 chunks to chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chefb\\AppData\\Local\\Temp\\ipykernel_19632\\3269549278.py:25: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "def generate_vector_database():\n",
    "    \"\"\"\n",
    "    Function to generate vector database in Chroma from provided documents\n",
    "    \"\"\"\n",
    "    documents = load_documents()\n",
    "    chunks = split_text_into_chunks(documents)\n",
    "    save_to_chroma(chunks)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Generate the vector database\n",
    "generate_vector_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter prompt: \n"
     ]
    }
   ],
   "source": [
    "print(\"Enter prompt: \")\n",
    "query_text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - - \n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chefb\\Desktop\\learning\\rag-technical-test\\.venv_311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\chefb\\AppData\\Local\\Temp\\ipykernel_21392\\500487773.py:14: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
      "C:\\Users\\chefb\\AppData\\Local\\Temp\\ipykernel_21392\\500487773.py:38: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response_text = model.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bruder, ein neuronales Netz, kurz auch ein neuronales Netzwerk, ist eine Art künstliche Intelligenz, die Fähigkeiten wie Lernen und Entscheidungsprozesse nachahmen kann. Im Kontext der überarbeiteten Fragestellung ist ein neuronales Netzwerk eine Methode im Bereich der künstlichen Intelligenz, welche dazu genutzt wird, Muster in Daten zu erkennen und Vorhersagen oder Entscheidungen aus diesen Mustern zu treffen.\n",
      "\n",
      "Neuronale Netze ähneln dem menschlichen Gehirn in ihrer Struktur. Sie bestehen aus mehreren Verarbeitungsschichten - sogenannten Neuronen -, die Informationen durchgehen. Dieser Datenfluss ist einfacher als in lebenden Organismen, weil es sich um eine technische Imitation bezüglich Funktionsweise handelt und keine komplexen neuronalen Prozesse simuliert wird.\n",
      "\n",
      "Im Steam von überwachtem Lernen, wie erwähnt in der gegebenen Zusammenfassung, werden neuronale Netze trainiert. Dies bedeutet, dass man mit markierten Datensätzen zugunsten einer bestimmten Zielfunktion lernt – beispielsweise ein Netzwerk, das mit Bild- oder Textdaten gelernet wurde, um über welches Resultat aufgrund dieser Daten urteilen kann.\n",
      "\n",
      "Der spezielle Vorzug von neuronalen Netzen, im Vergleich zu zuvor angewandten Lernmethoden wie Rekurrenten Neuronalen Netzen (RNNs) liegt in ihrer Fähigkeit, nicht nur Textprozesse, sondern auch komplexe Aufgaben in mehrsprachigen Szenarien zu bewältigen. RNNs waren eine früher auftauchende Methode, sie haben zwar bereits große Fortschritte gemacht, aber neuronale Netze ermöglichen aufgrund der verbesserten Struktur und Funktionsweise, sogar bessere Ergebnisse abzu- bzw. der jeweiligen Eingabe auf die entsprechende Ausgabe zu liefern.\n",
      "\n",
      "Zusammenfassend kann man sagen, dass ein neuronales Netz eine technologische Methode ist, welche auf der menschlichen Gehirnfunktion basiert und dazu dient, Informationen effizient verarbeiten und darauf basierende Entscheidungen oder Vorhersagen treffen zu können.\n"
     ]
    }
   ],
   "source": [
    "def query_rag(query_text):\n",
    "    \"\"\"\n",
    "    Query a RAG system using Chroma database and HuggingFace\n",
    "    args:\n",
    "    query_text(str): The text to query the RAG system with\n",
    "    returns:\n",
    "    formatted_response(str): Formatted response including the generated text\n",
    "    response_text(str): The generated response text\n",
    "    \"\"\"\n",
    "    # Use the same embedding function as before when creating a Chroma database\n",
    "    embedding_function = HuggingFaceEmbeddings()\n",
    "\n",
    "    # Prepare database\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Retrieving the context from the DB using similiarity search\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "\n",
    "    # Combine context from matching documents\n",
    "    context_text = \"\\n\\n - - \\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "    # Create prompt template using context and query text\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    # Initialize HuggingFace chat model deepseek\n",
    "    llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    "    )\n",
    "\n",
    "    model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "    # Generate response text from given chat model based on the prompt\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    # Get sources of the matching documents\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "\n",
    "    # Format and return response including generated text and sources\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "\n",
    "    return formatted_response, response_text\n",
    "\n",
    "formatted_response, response_text = query_rag(query_text)\n",
    "print(response_text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
